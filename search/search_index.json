{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/","text":"StatefulSets Using StatefulSets We need statefulSets for the following Unique network identifiers. Persistent storage. Deployments. Scaling. Rolling Updates. Cassandra Statefulsets Object Specs From kubernetes.io Pod Selector Ordinal Index Network ID Pod Identity Pod Name Label Pod Management Policies OrderedReady Pod Management Parallel Pod Management Update Strategies On Delete Rolling Updates Forced Rollback apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: \"nginx\" replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"my-storage-class\" resources: requests: storage: 1Gi Installing Cassandra Create cassandra headless object headless.yml apiVersion: v1 kind: Service metadata: labels: app: cassandra name: cassandra spec: clusterIP: None ports: - port: 9042 selector: app: cassandra Apply the object file kubectl apply -f headless.yml Create some persistant volumes https://kubernetes.io/docs/concepts/storage/persistent-volumes/ Create a strorage class object for the PV csc.yml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: csc provisioner: yours_elvis_cloud/volume_path parameters: repl: \"2\" priority_io: \"high\" group: \"cvg\" fg: \"true\" Apply the config kubectl apply -f csc.yml Create a 4 Replicasets Statefulsets cassandra statefull.yml apiVersion: \"apps/v1\" kind: StatefulSet metadata: name: cassandra spec: serviceName: cassandra selector: matchLabels: app: cassandra replicas: 3 template: metadata: labels: app: cassandra spec: # Use the stork scheduler to enable more efficient placement of the pods schedulerName: stork containers: - name: cassandra image: gcr.io/google-samples/cassandra:v12 imagePullPolicy: Always ports: - containerPort: 7000 name: intra-node - containerPort: 7001 name: tls-intra-node - containerPort: 7199 name: jmx - containerPort: 9042 name: cql resources: limits: cpu: \"500m\" memory: 1Gi requests: cpu: \"500m\" memory: 1Gi securityContext: capabilities: add: - IPC_LOCK lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"PID=$(pidof java) && kill $PID && while ps -p $PID > /dev/null; do sleep 1; done\"] env: - name: MAX_HEAP_SIZE value: 512M - name: HEAP_NEWSIZE value: 100M - name: CASSANDRA_SEEDS value: \"cassandra-0.cassandra.default.svc.cluster.local\" - name: CASSANDRA_CLUSTER_NAME value: \"K8Demo\" - name: CASSANDRA_DC value: \"DC1-K8Demo\" - name: CASSANDRA_RACK value: \"Rack1-K8Demo\" - name: CASSANDRA_AUTO_BOOTSTRAP value: \"false\" - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace readinessProbe: exec: command: - /bin/bash - -c - /ready-probe.sh initialDelaySeconds: 15 timeoutSeconds: 5 # These volume mounts are persistent. They are like inline claims, # but not exactly because the names need to match exactly one of # the stateful pod volumes. volumeMounts: - name: cassandra-data mountPath: /var/lib/cassandra # These are converted to volume claims by the controller # and mounted at the paths mentioned above. volumeClaimTemplates: - metadata: name: cassandra-data annotations: volume.beta.kubernetes.io/storage-class: csc spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi Apply the config kubectl apply -f statefull.yml Scaling Cassandra Scaling Cassandra from 4 to 5 and the other way round kubectl scale sts cassandra --replicas=5 Failover Pods Assuming there is data living inside cassandra, use the nodetool utility to figure out which node is hosting the data in cassandra nodetool getendpoints somedb emp 000000 Use kubectl cordon utility to force the identified node to reschedule pods to another node Failover Nodes Use the kubectl delete drain and `kubectl delete node' which will cause the stateful set to reschedule resources to other nodes","title":"StatefulSets"},{"location":"about/#statefulsets","text":"","title":"StatefulSets"},{"location":"about/#using-statefulsets","text":"We need statefulSets for the following Unique network identifiers. Persistent storage. Deployments. Scaling. Rolling Updates.","title":"Using StatefulSets"},{"location":"about/#cassandra-statefulsets-object-specs-from-kubernetesio","text":"Pod Selector Ordinal Index Network ID Pod Identity Pod Name Label Pod Management Policies OrderedReady Pod Management Parallel Pod Management Update Strategies On Delete Rolling Updates Forced Rollback apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: \"nginx\" replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"my-storage-class\" resources: requests: storage: 1Gi","title":"Cassandra Statefulsets Object Specs From kubernetes.io"},{"location":"about/#installing-cassandra","text":"Create cassandra headless object headless.yml apiVersion: v1 kind: Service metadata: labels: app: cassandra name: cassandra spec: clusterIP: None ports: - port: 9042 selector: app: cassandra Apply the object file kubectl apply -f headless.yml Create some persistant volumes https://kubernetes.io/docs/concepts/storage/persistent-volumes/ Create a strorage class object for the PV csc.yml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: csc provisioner: yours_elvis_cloud/volume_path parameters: repl: \"2\" priority_io: \"high\" group: \"cvg\" fg: \"true\" Apply the config kubectl apply -f csc.yml Create a 4 Replicasets Statefulsets cassandra statefull.yml apiVersion: \"apps/v1\" kind: StatefulSet metadata: name: cassandra spec: serviceName: cassandra selector: matchLabels: app: cassandra replicas: 3 template: metadata: labels: app: cassandra spec: # Use the stork scheduler to enable more efficient placement of the pods schedulerName: stork containers: - name: cassandra image: gcr.io/google-samples/cassandra:v12 imagePullPolicy: Always ports: - containerPort: 7000 name: intra-node - containerPort: 7001 name: tls-intra-node - containerPort: 7199 name: jmx - containerPort: 9042 name: cql resources: limits: cpu: \"500m\" memory: 1Gi requests: cpu: \"500m\" memory: 1Gi securityContext: capabilities: add: - IPC_LOCK lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"PID=$(pidof java) && kill $PID && while ps -p $PID > /dev/null; do sleep 1; done\"] env: - name: MAX_HEAP_SIZE value: 512M - name: HEAP_NEWSIZE value: 100M - name: CASSANDRA_SEEDS value: \"cassandra-0.cassandra.default.svc.cluster.local\" - name: CASSANDRA_CLUSTER_NAME value: \"K8Demo\" - name: CASSANDRA_DC value: \"DC1-K8Demo\" - name: CASSANDRA_RACK value: \"Rack1-K8Demo\" - name: CASSANDRA_AUTO_BOOTSTRAP value: \"false\" - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace readinessProbe: exec: command: - /bin/bash - -c - /ready-probe.sh initialDelaySeconds: 15 timeoutSeconds: 5 # These volume mounts are persistent. They are like inline claims, # but not exactly because the names need to match exactly one of # the stateful pod volumes. volumeMounts: - name: cassandra-data mountPath: /var/lib/cassandra # These are converted to volume claims by the controller # and mounted at the paths mentioned above. volumeClaimTemplates: - metadata: name: cassandra-data annotations: volume.beta.kubernetes.io/storage-class: csc spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi Apply the config kubectl apply -f statefull.yml","title":"Installing Cassandra"},{"location":"about/#scaling-cassandra","text":"Scaling Cassandra from 4 to 5 and the other way round kubectl scale sts cassandra --replicas=5","title":"Scaling Cassandra"},{"location":"about/#failover-pods","text":"Assuming there is data living inside cassandra, use the nodetool utility to figure out which node is hosting the data in cassandra nodetool getendpoints somedb emp 000000 Use kubectl cordon utility to force the identified node to reschedule pods to another node","title":"Failover Pods"},{"location":"about/#failover-nodes","text":"Use the kubectl delete drain and `kubectl delete node' which will cause the stateful set to reschedule resources to other nodes","title":"Failover Nodes"}]}